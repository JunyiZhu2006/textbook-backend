from flask import Flask, request, jsonify, send_file, Response
from flask_cors import CORS
import os
from werkzeug.utils import secure_filename
import fitz  # PyMuPDF
from openai import OpenAI
import deepl
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.docstore.document import Document
from dotenv import load_dotenv
import json
from docx import Document as DocxDocument
from docx.shared import Pt, RGBColor, Inches
from docx.enum.text import WD_ALIGN_PARAGRAPH
from pdf2image import convert_from_path
import io
import re
from collections import Counter
import time

load_dotenv()

app = Flask(__name__)
CORS(app)

# é…ç½®
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
ALLOWED_EXTENSIONS = {'pdf'}

os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024

# åˆå§‹åŒ–æœåŠ¡
print("\n" + "="*60)
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æœåŠ¡...")
print("="*60)

# DeepL
deepl_key = os.getenv('DEEPL_API_KEY')
if not deepl_key:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° DEEPL_API_KEY")
    exit(1)

try:
    translator = deepl.Translator(deepl_key)
    usage = translator.get_usage()
    print(f"âœ… DeepL ç¿»è¯‘å™¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"   å·²ä½¿ç”¨: {usage.character.count:,} / {usage.character.limit:,} å­—ç¬¦")
except Exception as e:
    print(f"âŒ DeepL åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    exit(1)

# OpenAI
openai_key = os.getenv('OPENAI_API_KEY')
if not openai_key:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° OPENAI_API_KEY")
    exit(1)

try:
    client = OpenAI(api_key=openai_key)
    embeddings = OpenAIEmbeddings()
    print("âœ… OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ OpenAI åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    exit(1)

print("="*60)

vector_stores = {}
conversation_history = {}

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def extract_text_with_format_tags(pdf_path):
    """
    ä»PDFæå–æ–‡æœ¬ï¼ŒåµŒå…¥æ ¼å¼æ ‡ç­¾
    æ ‡ç­¾æ ¼å¼ï¼š##H1##æ–‡æœ¬##/H1## ï¼ˆç”¨##é¿å…è¢«ç¿»è¯‘ï¼‰
    """
    doc = fitz.open(pdf_path)
    pages_content = []
    print(f"ğŸ“– PDF æ€»é¡µæ•°: {len(doc)}")
    
    for page_num in range(len(doc)):
        page = doc[page_num]
        
        # æå–æ–‡æœ¬å—
        blocks_data = []
        dict_blocks = page.get_text("dict")["blocks"]
        
        # æ”¶é›†å­—ä½“å¤§å°ç”¨äºåˆ†æ
        all_font_sizes = []
        for block in dict_blocks:
            if block['type'] == 0:  # æ–‡æœ¬å—
                for line in block.get('lines', []):
                    for span in line.get('spans', []):
                        text = span.get('text', '').strip()
                        if text:
                            font_size = span.get('size', 12)
                            font_name = span.get('font', '')
                            flags = span.get('flags', 0)
                            is_bold = bool(flags & 2**4) or 'bold' in font_name.lower()
                            is_italic = bool(flags & 2**1) or 'italic' in font_name.lower()
                            x0 = span['bbox'][0]
                            
                            blocks_data.append({
                                'text': text,
                                'font_size': font_size,
                                'is_bold': is_bold,
                                'is_italic': is_italic,
                                'x0': x0
                            })
                            all_font_sizes.append(font_size)
        
        if not blocks_data:
            continue
        
        # åˆ†æå­—ä½“å¤§å°åˆ†å¸ƒ
        size_counts = Counter(all_font_sizes)
        sorted_sizes = sorted(set(all_font_sizes), reverse=True)
        
        # ç¡®å®šæ ‡é¢˜é˜ˆå€¼
        if len(sorted_sizes) >= 3:
            h1_threshold = sorted_sizes[0]
            h2_threshold = sorted_sizes[1]
            h3_threshold = sorted_sizes[2]
        elif len(sorted_sizes) >= 2:
            h1_threshold = sorted_sizes[0]
            h2_threshold = sorted_sizes[1]
            h3_threshold = sorted_sizes[1]
        else:
            h1_threshold = sorted_sizes[0] if sorted_sizes else 14
            h2_threshold = h1_threshold
            h3_threshold = h1_threshold
        
        # ç¡®å®šåŸºå‡†å·¦è¾¹è·
        min_x = min(b['x0'] for b in blocks_data) if blocks_data else 0
        
        # ç»„è£…å¸¦æ ‡ç­¾çš„æ–‡æœ¬
        tagged_lines = []
        for block in blocks_data:
            text = block['text']
            font_size = block['font_size']
            is_bold = block['is_bold']
            x0 = block['x0']
            
            # è®¡ç®—ç¼©è¿›çº§åˆ«
            indent = int((x0 - min_x) / 30)  # æ¯30åƒç´ ä¸€çº§
            
            # ç¡®å®šæ ¼å¼æ ‡ç­¾
            if font_size >= h1_threshold - 0.5:
                tag = 'H1'
            elif font_size >= h2_threshold - 0.5:
                tag = 'H2'
            elif font_size >= h3_threshold - 0.5:
                tag = 'H3'
            elif is_bold:
                tag = 'B'
            elif indent > 0:
                tag = f'INDENT{indent}'
            else:
                tag = 'P'  # æ™®é€šæ®µè½
            
            # åµŒå…¥æ ‡ç­¾ï¼š##TAG##æ–‡æœ¬##/TAG##
            tagged_text = f"##{tag}##{text}##/{tag}##"
            tagged_lines.append(tagged_text)
        
        # åˆå¹¶æˆå®Œæ•´æ–‡æœ¬
        full_tagged_text = '\n'.join(tagged_lines)
        
        # ä¹Ÿä¿å­˜ä¸€ä»½çº¯æ–‡æœ¬ï¼ˆç”¨äºå‘é‡æ•°æ®åº“ï¼‰
        plain_text = '\n'.join([b['text'] for b in blocks_data])
        
        pages_content.append({
            'page': page_num + 1,
            'tagged_text': full_tagged_text,  # å¸¦æ ‡ç­¾çš„æ–‡æœ¬
            'plain_text': plain_text  # çº¯æ–‡æœ¬
        })
    
    doc.close()
    print(f"ğŸ“„ æˆåŠŸæå– {len(pages_content)} é¡µå†…å®¹ï¼ˆå¸¦æ ¼å¼æ ‡ç­¾ï¼‰")
    return pages_content

def translate_with_deepl(tagged_text):
    """
    ç¿»è¯‘å¸¦æ ‡ç­¾çš„æ–‡æœ¬
    DeepLåº”è¯¥ä¼šä¿ç•™##TAG##æ ‡ç­¾ï¼Œåªç¿»è¯‘ä¸­é—´çš„æ–‡æœ¬
    """
    try:
        max_length = 4000  # ç•™ä¸€äº›ç©ºé—´ç»™æ ‡ç­¾
        if len(tagged_text) > max_length:
            # æŒ‰è¡Œåˆ†å‰²ï¼ˆæ¯è¡Œæ˜¯ä¸€ä¸ªæ ‡ç­¾å—ï¼‰
            lines = tagged_text.split('\n')
            translated_lines = []
            current_batch = []
            current_length = 0
            
            for line in lines:
                if current_length + len(line) > max_length and current_batch:
                    # ç¿»è¯‘å½“å‰æ‰¹æ¬¡
                    batch_text = '\n'.join(current_batch)
                    result = translator.translate_text(batch_text, target_lang="ZH")
                    translated_lines.append(result.text)
                    current_batch = [line]
                    current_length = len(line)
                else:
                    current_batch.append(line)
                    current_length += len(line)
            
            # ç¿»è¯‘æœ€åä¸€æ‰¹
            if current_batch:
                batch_text = '\n'.join(current_batch)
                result = translator.translate_text(batch_text, target_lang="ZH")
                translated_lines.append(result.text)
            
            return '\n'.join(translated_lines)
        else:
            result = translator.translate_text(tagged_text, target_lang="ZH")
            return result.text
    except Exception as e:
        print(f"âš ï¸ DeepL ç¿»è¯‘é”™è¯¯: {str(e)}")
        return tagged_text

def generate_summary(text):
    """ç”Ÿæˆæ‘˜è¦"""
    try:
        max_length = 2000
        if len(text) > max_length:
            text = text[:max_length]
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼Œ2-3å¥è¯ï¼Œçªå‡ºæ ¸å¿ƒæ¦‚å¿µã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0.5,
            max_tokens=150
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"âš ï¸ æ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}")
        return "[æ‘˜è¦ç”Ÿæˆå¤±è´¥]"

def polish_translation_with_gpt(tagged_translation, original_text):
    """
    GPTæ¶¦è‰²ï¼Œä¿æŒæ ‡ç­¾ä¸å˜
    """
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€åä¸¥è°¨çš„åŒè¯­å­¦æœ¯ç¼–è¾‘ã€‚
ä½ å°†æ”¶åˆ°ä¸€æ®µå¸¦æ ¼å¼æ ‡ç­¾çš„ä¸­æ–‡ç¿»è¯‘æ–‡æœ¬ã€‚æ ‡ç­¾æ ¼å¼ä¸ºï¼š##TAG##æ–‡æœ¬##/TAG##

ä½ çš„ä»»åŠ¡ï¼š
1. å¯¹æ ‡ç­¾ä¹‹é—´çš„ä¸­æ–‡å†…å®¹è¿›è¡Œæœ€å°æ¶¦è‰²ï¼Œä½¿å…¶è‡ªç„¶æµç•…ï¼›
2. **ç»å¯¹ä¸èƒ½åˆ é™¤ã€ä¿®æ”¹æˆ–ç§»åŠ¨ä»»ä½•##TAG##æ ‡ç­¾**ï¼›
3. åªæ¶¦è‰²ä¸­æ–‡å†…å®¹ï¼Œä¸æ”¹å˜æ ‡ç­¾ä½ç½®å’Œæ•°é‡ï¼›
4. ä¸å¾—å¢åˆ å¥å­ï¼Œåªä¼˜åŒ–æªè¾ã€‚

é‡è¦ï¼šæ ‡ç­¾å¿…é¡»åŸæ ·ä¿ç•™ï¼ŒåŒ…æ‹¬##ç¬¦å·ã€‚
è¾“å‡ºæ ¼å¼ï¼šç›´æ¥è¿”å›æ¶¦è‰²åçš„æ–‡æœ¬ï¼Œæ ‡ç­¾å®Œæ•´ä¿ç•™ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"è¯·æ¶¦è‰²ä»¥ä¸‹å¸¦æ ‡ç­¾çš„ç¿»è¯‘ï¼ˆä¿æŒæ‰€æœ‰##TAG##æ ‡ç­¾ä¸å˜ï¼‰ï¼š\n\n{tagged_translation}"
                }
            ],
            temperature=0.3,
            max_tokens=3000
        )
        
        polished = response.choices[0].message.content.strip()
        
        # éªŒè¯æ ‡ç­¾æ•°é‡æ˜¯å¦ä¸€è‡´
        orig_tag_count = tagged_translation.count('##')
        new_tag_count = polished.count('##')
        
        if orig_tag_count != new_tag_count:
            print(f"   âš ï¸ è­¦å‘Šï¼šæ ‡ç­¾æ•°é‡ä¸ä¸€è‡´ï¼åŸ{orig_tag_count}ä¸ªï¼Œç°{new_tag_count}ä¸ª")
            print(f"   â†’ ä½¿ç”¨æœªæ¶¦è‰²ç‰ˆæœ¬")
            return tagged_translation
        
        return polished
    except Exception as e:
        print(f"âš ï¸ GPT æ¶¦è‰²é”™è¯¯: {str(e)}")
        return tagged_translation

def parse_tagged_text_to_word(doc, tagged_text):
    """
    è§£æå¸¦æ ‡ç­¾çš„æ–‡æœ¬ï¼Œåº”ç”¨æ ¼å¼åˆ°Word
    """
    # æŒ‰è¡Œåˆ†å‰²ï¼ˆæ¯è¡Œä¸€ä¸ªæ ‡ç­¾å—ï¼‰
    lines = tagged_text.split('\n')
    
    for line in lines:
        if not line.strip():
            continue
        
        # æå–æ ‡ç­¾å’Œæ–‡æœ¬
        # æ ¼å¼ï¼š##TAG##æ–‡æœ¬##/TAG##
        pattern = r'##([^#]+)##(.+?)##/\1##'
        match = re.search(pattern, line)
        
        if not match:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå½“ä½œæ™®é€šæ–‡æœ¬
            doc.add_paragraph(line)
            continue
        
        tag = match.group(1)
        text = match.group(2)
        
        # æ ¹æ®æ ‡ç­¾ç±»å‹åº”ç”¨æ ¼å¼
        if tag == 'H1':
            doc.add_heading(text, level=1)
        elif tag == 'H2':
            doc.add_heading(text, level=2)
        elif tag == 'H3':
            doc.add_heading(text, level=3)
        elif tag == 'B':
            para = doc.add_paragraph()
            run = para.add_run(text)
            run.font.bold = True
            run.font.size = Pt(11)
        elif tag.startswith('INDENT'):
            # æå–ç¼©è¿›çº§åˆ«
            indent_level = int(tag.replace('INDENT', '')) if len(tag) > 6 else 1
            para = doc.add_paragraph(text)
            para.paragraph_format.left_indent = Inches(0.3 * indent_level)
            for run in para.runs:
                run.font.size = Pt(11)
        else:  # P æˆ–å…¶ä»–
            para = doc.add_paragraph(text)
            for run in para.runs:
                run.font.size = Pt(11)

def create_formatted_word_doc(pdf_path, pages_data, output_path):
    """åˆ›å»ºå¸¦æ ¼å¼çš„Wordæ–‡æ¡£"""
    try:
        doc = DocxDocument()
        
        # æ ‡é¢˜
        title = doc.add_heading('æ•™æç¿»è¯‘ - ä¸­è‹±å¯¹ç…§ï¼ˆå®Œç¾æ ¼å¼ç‰ˆï¼‰', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        subtitle = doc.add_paragraph('åŸæ–‡PDFæˆªå›¾ + ä¸­æ–‡å®Œæ•´æ ¼å¼è¿˜åŸ')
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        doc.add_paragraph()
        
        print(f"ğŸ–¼ï¸ å¼€å§‹è½¬æ¢PDFä¸ºå›¾ç‰‡...")
        images = convert_from_path(pdf_path, dpi=150, fmt='png')
        print(f"âœ… æˆåŠŸè½¬æ¢ {len(images)} é¡µä¸ºå›¾ç‰‡")
        
        for idx, page_data in enumerate(pages_data):
            page_num = page_data['page'] - 1
            
            # é¡µç æ ‡é¢˜
            doc.add_heading(f'ç¬¬ {page_data["page"]} é¡µ', level=1)
            
            # æ‘˜è¦
            doc.add_heading('ğŸ“ å†…å®¹æ‘˜è¦', level=2)
            summary_para = doc.add_paragraph(page_data['summary'])
            for run in summary_para.runs:
                run.font.size = Pt(11)
                run.font.color.rgb = RGBColor(0, 102, 204)
            
            # è‹±æ–‡åŸæ–‡ï¼ˆPDFæˆªå›¾ï¼‰
            doc.add_heading('ğŸ‡¬ğŸ‡§ è‹±æ–‡åŸæ–‡ï¼ˆåŸå§‹æ’ç‰ˆï¼‰', level=2)
            if page_num < len(images):
                img_buffer = io.BytesIO()
                images[page_num].save(img_buffer, format='PNG')
                img_buffer.seek(0)
                doc.add_picture(img_buffer, width=Inches(6))
            
            # ä¸­æ–‡ç¿»è¯‘ï¼ˆå¸¦æ ¼å¼ï¼‰
            doc.add_heading('ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç¿»è¯‘ï¼ˆæ ¼å¼è¿˜åŸï¼‰', level=2)
            
            # è§£æå¸¦æ ‡ç­¾çš„ç¿»è¯‘æ–‡æœ¬
            parse_tagged_text_to_word(doc, page_data['translation'])
            
            # åˆ†é¡µç¬¦
            if idx < len(pages_data) - 1:
                doc.add_page_break()
            
            print(f"   âœ… ç¬¬ {page_data['page']} é¡µå®Œæˆ")
        
        doc.save(output_path)
        print(f"ğŸ’¾ Word æ–‡æ¡£å·²ä¿å­˜: {output_path}")
        return True
    except Exception as e:
        print(f"âš ï¸ Word æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def find_relevant_snippet(full_text, query, context_chars=200):
    """åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„ç‰‡æ®µ"""
    query_lower = query.lower()
    text_lower = full_text.lower()
    
    pos = text_lower.find(query_lower)
    
    if pos == -1:
        words = query.split()
        for word in words:
            pos = text_lower.find(word.lower())
            if pos != -1:
                break
    
    if pos == -1:
        return full_text[:context_chars * 2], 0, min(context_chars * 2, len(full_text))
    
    start = max(0, pos - context_chars)
    end = min(len(full_text), pos + len(query) + context_chars)
    
    snippet = full_text[start:end]
    highlight_start = pos - start
    highlight_end = highlight_start + len(query)
    
    return snippet, highlight_start, highlight_end

@app.route('/')
def home():
    return jsonify({
        "message": "ğŸ‰ æ™ºèƒ½æ•™æåŠ©æ‰‹åç«¯è¿è¡ŒæˆåŠŸï¼",
        "project": "å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨",
        "version": "å®Œç¾æ ¼å¼ç‰ˆï¼ˆæ ‡ç­¾æ–¹æ¡ˆï¼‰",
        "services": {
            "pymupdf": "âœ… æ ¼å¼æå–",
            "deepl": "âœ… å¿ å®ç¿»è¯‘",
            "openai": "âœ… æ¶¦è‰²+æ ¡éªŒ",
            "format_tags": "âœ… æ ‡ç­¾åµŒå…¥æ–¹æ¡ˆ",
            "features": "âœ… H1/H2/H3 + ç²—ä½“ + ç¼©è¿›"
        }
    })

@app.route('/test-deepl')
def test_deepl():
    try:
        result = translator.translate_text("Hello, world!", target_lang="ZH")
        usage = translator.get_usage()
        return jsonify({
            "status": "success",
            "test_text": "Hello, world!",
            "translation": result.text,
            "usage": {
                "character_count": usage.character.count,
                "character_limit": usage.character.limit,
                "remaining": usage.character.limit - usage.character.count
            }
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

def generate_progress_updates(filepath, filename):
    """ç”Ÿæˆå™¨å‡½æ•°ï¼šå®æ—¶æ¨é€å¤„ç†è¿›åº¦"""
    try:
        # 1. æå–PDFæ–‡æœ¬
        yield f"data: {json.dumps({'stage': 'extract', 'message': 'æ­£åœ¨æå–PDFæ–‡æœ¬...', 'progress': 10})}\n\n"
        pages_content = extract_text_with_format_tags(filepath)
        total_pages = len(pages_content)
        max_pages = min(5, total_pages)
        
        yield f"data: {json.dumps({'stage': 'extract', 'message': f'æˆåŠŸæå– {total_pages} é¡µå†…å®¹', 'progress': 15})}\n\n"
        
        processed_pages = []
        documents = []
        
        # 2. å¤„ç†æ¯ä¸€é¡µ
        for i, page_data in enumerate(pages_content[:max_pages]):
            page_num = page_data['page']
            tagged_text = page_data['tagged_text']
            plain_text = page_data['plain_text']
            
            base_progress = 15 + (i * 70 // max_pages)
            
            # DeepL ç¿»è¯‘
            yield f"data: {json.dumps({'stage': 'translate', 'message': f'ç¬¬ {page_num} é¡µï¼šDeepL ç¿»è¯‘ä¸­...', 'progress': base_progress + 5, 'current_page': page_num, 'total_pages': max_pages})}\n\n"
            translation = translate_with_deepl(tagged_text)
            
            # GPT æ¶¦è‰²
            yield f"data: {json.dumps({'stage': 'polish', 'message': f'ç¬¬ {page_num} é¡µï¼šGPT æ¶¦è‰²ä¸­...', 'progress': base_progress + 10, 'current_page': page_num, 'total_pages': max_pages})}\n\n"
            translation = polish_translation_with_gpt(translation, plain_text)
            
            # ç”Ÿæˆæ‘˜è¦
            yield f"data: {json.dumps({'stage': 'summary', 'message': f'ç¬¬ {page_num} é¡µï¼šç”Ÿæˆæ‘˜è¦ä¸­...', 'progress': base_progress + 12, 'current_page': page_num, 'total_pages': max_pages})}\n\n"
            summary = generate_summary(plain_text)
            
            processed_page = {
                'page': page_num,
                'content': plain_text,
                'translation': translation,
                'summary': summary
            }
            processed_pages.append(processed_page)
            
            doc = Document(
                page_content=plain_text,
                metadata={
                    'page': page_num,
                    'translation': translation,
                    'summary': summary,
                    'source': filename
                }
            )
            documents.append(doc)
            
            yield f"data: {json.dumps({'stage': 'complete_page', 'message': f'ç¬¬ {page_num} é¡µå¤„ç†å®Œæˆ', 'progress': base_progress + 14, 'current_page': page_num, 'total_pages': max_pages})}\n\n"
        
        # 3. åˆ›å»ºå‘é‡æ•°æ®åº“
        yield f"data: {json.dumps({'stage': 'vectordb', 'message': 'åˆ›å»ºå‘é‡æ•°æ®åº“...', 'progress': 85})}\n\n"
        doc_id = filename.replace('.pdf', '')
        try:
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=embeddings,
                collection_name=doc_id
            )
            vector_stores[doc_id] = vector_store
        except Exception as e:
            print(f"âš ï¸ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {str(e)}")
        
        # 4. ä¿å­˜JSON
        yield f"data: {json.dumps({'stage': 'save_json', 'message': 'ä¿å­˜JSONæ–‡ä»¶...', 'progress': 88})}\n\n"
        output_json = os.path.join(OUTPUT_FOLDER, f"{doc_id}_processed.json")
        with open(output_json, 'w', encoding='utf-8') as f:
            json.dump(processed_pages, f, ensure_ascii=False, indent=2)
        
        # 5. ç”ŸæˆWordæ–‡æ¡£
        yield f"data: {json.dumps({'stage': 'word', 'message': 'ç”ŸæˆWordæ–‡æ¡£ï¼ˆè½¬æ¢PDFä¸ºå›¾ç‰‡ï¼‰...', 'progress': 90})}\n\n"
        output_docx = os.path.join(OUTPUT_FOLDER, f"{doc_id}_bilingual.docx")
        create_formatted_word_doc(filepath, processed_pages, output_docx)
        
        # å®Œæˆ
        yield f"data: {json.dumps({'stage': 'done', 'message': 'å¤„ç†å®Œæˆï¼', 'progress': 100, 'doc_id': doc_id, 'pages': processed_pages, 'total_pages': total_pages, 'processed_pages': len(processed_pages), 'json_file': f'/download/{doc_id}_processed.json', 'docx_file': f'/download/{doc_id}_bilingual.docx'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'stage': 'error', 'message': f'å¤„ç†å¤±è´¥: {str(e)}', 'progress': 0})}\n\n"

@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
    
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(filepath)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜: {filename}")
        print(f"{'='*60}\n")
        
        # è¿”å› SSE æµ
        return Response(
            generate_progress_updates(filepath, filename),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no'
            }
        )
    
    return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'}), 400

@app.route('/chat', methods=['POST'])
def chat():
    data = request.json
    question = data.get('question')
    doc_id = data.get('doc_id')
    
    if not question or not doc_id:
        return jsonify({'error': 'ç¼ºå°‘é—®é¢˜æˆ–æ–‡æ¡£ID'}), 400
    
    if doc_id not in vector_stores:
        return jsonify({'error': 'æœªæ‰¾åˆ°æ–‡æ¡£'}), 404
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¬ æ”¶åˆ°é—®é¢˜: {question}")
    print(f"{'='*60}\n")
    
    if doc_id not in conversation_history:
        conversation_history[doc_id] = []
    
    history = conversation_history[doc_id]
    vector_store = vector_stores[doc_id]
    
    relevant_docs = vector_store.similarity_search(question, k=3)
    print(f"ğŸ“š æ‰¾åˆ° {len(relevant_docs)} ä¸ªç›¸å…³æ®µè½\n")
    
    context_parts = []
    for doc in relevant_docs:
        context_parts.append(
            f"ç¬¬{doc.metadata['page']}é¡µ:\n"
            f"åŸæ–‡: {doc.page_content[:500]}\n"
            f"ç¿»è¯‘: {doc.metadata['translation'][:500]}"
        )
    
    context = "\n\n".join(context_parts)
    
    messages = [
        {
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä¸ªéå¸¸æœ‰è€å¿ƒçš„å­¦ä¹ åŠ©æ‰‹ï¼Œåƒä¸€ä¸ªå¥½æœ‹å‹ä¸€æ ·å¸®åŠ©å­¦ç”Ÿç†è§£æ•™æã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ç”¨å£è¯­åŒ–ã€è½»æ¾çš„æ–¹å¼è§£é‡Š
2. ç”¨ Markdown æ ¼å¼ç»„ç»‡ç­”æ¡ˆï¼ˆç²—ä½“ã€åˆ—è¡¨ã€å¼•ç”¨ã€ä»£ç å—ï¼‰
3. å¾ªåºæ¸è¿›ï¼šå…ˆç®€å•è§£é‡Šï¼Œå†è¯¦ç»†è¯´æ˜ï¼Œæœ€åç»™ä¾‹å­
4. å½“å­¦ç”Ÿè¯´"ä¸æ‡‚"æ—¶ï¼Œæ¢æ›´ç®€å•çš„æ¯”å–»
5. å¼•ç”¨é¡µç 

è®°ä½ï¼šè®©å­¦ç”ŸçœŸæ­£ç†è§£ï¼Œè€Œä¸æ˜¯ç‚«è€€çŸ¥è¯†ï¼"""
        }
    ]
    
    for h in history[-6:]:
        messages.append({"role": "user", "content": h["question"]})
        messages.append({"role": "assistant", "content": h["answer"]})
    
    messages.append({
        "role": "user",
        "content": f"æ•™æå†…å®¹:\n{context}\n\né—®é¢˜: {question}"
    })
    
    print(f"ğŸ¤” æ­£åœ¨ç”Ÿæˆå‹å¥½çš„å›ç­”...")
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            temperature=0.8,
            max_tokens=800
        )
        
        answer = response.choices[0].message.content
        print(f"âœ… å›ç­”ç”Ÿæˆå®Œæˆ\n")
        
        history.append({
            "question": question,
            "answer": answer
        })
        
        if len(history) > 10:
            history = history[-10:]
        conversation_history[doc_id] = history
        
    except Exception as e:
        print(f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}\n")
        answer = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™"
    
    references = []
    for doc in relevant_docs:
        snippet, highlight_start, highlight_end = find_relevant_snippet(
            doc.page_content,
            question
        )
        
        references.append({
            'page': doc.metadata['page'],
            'content': doc.page_content[:500],
            'translation': doc.metadata['translation'][:500],
            'summary': doc.metadata['summary'],
            'snippet': snippet,
            'highlight_start': highlight_start,
            'highlight_end': highlight_end
        })
    
    return jsonify({
        'answer': answer,
        'references': references
    })

@app.route('/download/<filename>')
def download_file(filename):
    return send_file(
        os.path.join(OUTPUT_FOLDER, filename),
        as_attachment=True
    )

@app.route('/usage')
def check_usage():
    try:
        usage = translator.get_usage()
        remaining = usage.character.limit - usage.character.count
        return jsonify({
            "status": "ok",
            "character": {
                "count": usage.character.count,
                "limit": usage.character.limit,
                "remaining": remaining,
                "percentage": round((usage.character.count / usage.character.limit) * 100, 2)
            },
            "limit_reached": usage.any_limit_reached
        })
    except Exception as e:
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == '__main__':
    print("\n" + "="*60)
    print("ğŸš€ æ™ºèƒ½æ•™æåŠ©æ‰‹ - å®Œç¾æ ¼å¼ç‰ˆï¼ˆæ ‡ç­¾æ–¹æ¡ˆï¼‰")
    print("   å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨")
    print("="*60)
    print("ğŸ“ åç«¯åœ°å€: http://localhost:5000")
    print("ğŸ”§ æµ‹è¯•ç¿»è¯‘: http://localhost:5000/test-deepl")
    print("ğŸ“Š æŸ¥çœ‹ç”¨é‡: http://localhost:5000/usage")
    print("âœ¨ æ–¹æ¡ˆ: æ ‡ç­¾åµŒå…¥ â†’ DeepLç¿»è¯‘ â†’ GPTæ¶¦è‰² â†’ æ ¼å¼è¿˜åŸ")
    print("ğŸ¨ æ ¼å¼: H1/H2/H3æ ‡é¢˜ + ç²—ä½“ + ç¼©è¿›")
    print("="*60 + "\n")
    
 if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)

